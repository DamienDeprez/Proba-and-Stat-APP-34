\documentclass[a4paper]{article}

\usepackage{minted}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}      
\usepackage[english]{babel}
\usepackage{array}
% Layout and figures
\usepackage[top=2.5cm,bottom=2.5cm,right=2.5cm,left=2.5cm]{geometry}
\usepackage{subfigure}
\usepackage{rotating}
\usepackage{caption}
% Units and numbers
\usepackage[squaren, Gray]{SIunits}
\usepackage{sistyle}
\usepackage[autolanguage]{numprint}
% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% Sets
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
% Links
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\usepackage{enumerate}
\usepackage{listings}
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

%\usepackage{titlesec}
% New commands
\newcommand{\matlab}{\textsc{Matlab}}
\newcommand{\annexe}{\part{Annexes}\appendix}
\newcommand{\biblioreport}[1]{\bibliographystyle{plain}\bibliography{#1}\nocite{*}}
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\fpart}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ffpart}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\fdpart}[3]{\frac{\partial^2 #1}{\partial #2\partial #3}}
\newcommand{\fdif}[2]{\frac{\dif #1}{\dif #2}}
\newcommand{\ffdif}[2]{\frac{\dif^2 #1}{\dif #2^2}}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

%\titleformat{\section}
%{\bf\fontsize{20.74}{20.74}\selectfont}
%{\bf\fontsize{20.74}{20.74}\selectfont \thesection \hspace{1 cm}}
%{0 pt}{}{}
%\titlespacing{\section}{10 pt}{10 pt}{20 pt}[10 pt]

\begin{document}
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\center 
\textsc{\Large Universit\'e Catholique de Louvain}\\[1cm] 
\textsc{\LARGE LFSAB1105 - Probability and Statistics}\\[0.5cm] 

\HRule \\[0.4cm]
{ \huge \bfseries APP}\\ [0.4cm]
\HRule \\[0.1cm]
\vspace{1cm}
%LOGO UCL
\begin{figure}[ht]
\centering
\includegraphics [height=10cm] {img/ucl}
\end{figure}
\vspace{1cm}
%Author Name
\begin{minipage}{0.7\textwidth}
\begin{center}
\begin{tabular}{lc}
Laurent \textsc{Deleu} & 6040-13-00 \\
Nicolas \textsc{Delinte} & 4801-13-00\\
Damien \textsc{Deprez} & 2893-13-00 \\
Bastien \textsc{Gillon} & 5937-12-00\\
Abbas \textsc{Sliti} & 2485-13-00\\

\end{tabular}
\end{center}

\end{minipage}\\[1cm]

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large Ann\'ee acad\'emique 2016-2017}\\[0,25cm] 
{\large \'Ecole Polytechnique de Louvain}\\[1cm]

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\begin{center}
  \includegraphics[width = 20mm]{img/epl.jpg} \hfill
\end{center}
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Part 1 : Descriptive statistics}

\begin{enumerate}[(a)]

\item In order to find $t_{p}$ the quantile of order p such that $ P(T \leq t_{p}) = p $, we need to find the solution to the following equation.

$$F_{\alpha , \beta}(t) = \int_{0}^{t_{p}} f_{\alpha , \beta}(t') dt' = p $$
\\
Before integrating the density function, note that :

$$ \frac{\mathrm{d} }{\mathrm{d} x} (1-t^\alpha)^\beta = - \alpha \beta t^{\alpha-1} (1-t^\alpha)^{\beta -1}$$
\\
Thus,

$$F_{\alpha , \beta} (t_p) \quad = \int_{0}^{t_{p}} \alpha \beta t'^{\alpha-1} (1-t'^\alpha)^{\beta -1} dt' = \left [  -(1-t'^\alpha)^\beta \right ]^{t_{p}}_{0} = 1-(1-t_{p}^{\alpha})^\beta$$
\\
Then, the result can be easily found : $\quad t_p = (1-(1-p)^{\frac{1}{\beta}})^{\frac{1}{\alpha}}$

\item Let's apply the general expression of the moment of order k to our r.v.

\nonumber
\begin{equation} \label{eq1}
\begin{split}
\mathbb{E}(T^k) & = \int_{0}^{1} t^k \alpha \beta t^{\alpha -1} (1-t^\alpha)^{\beta-1} dt \\
 & = \beta \int_{0}^{1} \tau^{\frac{k}{\alpha}} (1-\tau)^{\beta -1} d\tau \\
 & = \beta \frac{\Gamma(\frac{k}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{k}{\alpha}+1+\beta)}
\end{split}
\end{equation}
\\
In order to obtain an expression similar to the hint given in the instructions, it is necessary to use a variable change : $\tau = t^\alpha$.

\item The previous results allow us to find some statistics describing our variable T.
\\
$\textbf{Mean :} \quad \mu = \mathbb{E}(T) = \beta \frac{\Gamma(\frac{1}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{1}{\alpha}+1+\beta)} $
\\
$\textbf{Variance :} \quad \sigma^2 = \mathbb{E}(T^2)-(\mathbb{E}(T))^2 = \beta\frac{\Gamma(\frac{2}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{2}{\alpha}+1+\beta)} - \beta^2 \frac{\Gamma(\frac{1}{\alpha}+1)^2\Gamma(\beta)^2}{\Gamma(\frac{1}{\alpha}+1+\beta)^2}$
\\
$\textbf{Median :} \quad t_{0.5} = (1-\frac{1}{2^{\frac{1}{\beta}}})^\frac{1}{\alpha}$
\\
$\textbf{Range :} \quad \textup{Range} = 1$ since $f$ is strictly positive $\forall t \in \left ]  0;1 \right [ $
\\
$\textbf{Interquartile range :} \quad \textup{IQR} = t_{0.75} - t_{0.25} = (1-\frac{1}{4^\frac{1}{\beta}})^\frac{1}{\alpha} - (1-(\frac{3}{4})^\frac{1}{\beta})^\frac{1}{\alpha}$
\\
$\textbf{Coefficient of variation :} \quad cv = \frac{\sigma}{\mu} = \frac{\sqrt{\beta\frac{\Gamma(\frac{2}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{2}{\alpha}+1+\beta)} - \beta^2 \frac{\Gamma(\frac{1}{\alpha}+1)^2\Gamma(\beta)^2}{\Gamma(\frac{1}{\alpha}+1+\beta)^2}}}{\beta \frac{\Gamma(\frac{1}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{1}{\alpha}+1+\beta)}}$

\item Let's fix our parameters to have an idea of the values of our statistics. For $\alpha_0 = 2$ and $\beta_0 = 4$, we obtain : $\mu = 0.4063$ , $\sigma^2 = 0.0349$ , $t_{0.5} = 0.3989$ , $\textup{IQR} = 0.2778$ and $cv = 0.4596$.

\item The probability that no more than halve of our 50 observations will have a value less than 0.5, can be found in two steps. First, we calculate the probability that an observation is less than 0.5:

$$P(T<0.5) = \int_{-\infty}^{0.5} = F(0.5) - F(0) = 0.6836 $$
\\
Let's define Y, the total number of successful observations. This new r.v. follows a binomial distribution with $p=0.6836$, our probability of success and $n=50$, our number of trials. Our second step is to calculate the probability that Y cannot exceed 25.

$$P(X \leq 25) = \sum_{k=0}^{25} C_{k}^{50}p^{k}(1-p)^{50-k} = 0.0052$$

To obtain such a result, it is necessary to make a long summation which can be avoided if we use the Central Limit Theorem. This method uses the fact that the r.v. $\frac{Y-\mu}{\sigma / \sqrt{n}}$ follows a standard normal distribution:

$$P(Y \leq 25) = P\left ( \frac{Y-np}{\sqrt{np(1-p)}} \leq \frac{25-50\cdot0.6836}{\sqrt{50\cdot0.6836\cdot(1-0.6836)}} \right ) \approx P(Z < -2.7915) = 0.0026$$

Our approximated result is not as close as we could have expected, but it is still in the same order of magnitude. This is caused by the huge gap between 25 and 34.2, the mean of our binomial distribution, which depends on our choice of parameters. The further we get away from this mean, the less accurate the approximation will be.

\item From one generated $idd$ sample of size $n = 20$, we observed the following statistics : $\mu = 0.3919$, $\sigma^2 = 0.0432$, $t_0.5 = 0.3922$, $\textup{IQR} = 0.3851$ and $cv = 0.5302$.
These values are very close to the ones calculated previously.

\item As we progressively increase the sample size, the observed statistics approach their theoretical value. We may therefore conclude that the approximation U $\sim$ Unif[0, 1] tends to converge for an increasing size of the sample.

\begin{figure}
    \centering
  \includegraphics[width = 10cm]{img/values.png}
  \caption{Results of the calculations in (f) for n=40,60,80,...,10000.}
\end{figure}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Part 2 : Estimation}

\begin{enumerate}[(a)]

\item A first estimator could be taking the rounded value at $ 95\% $ of the ordered sample:
$$\hat{q}_s = T_{[0.95\cdot n]}$$
This estimator would be unbiased if our sample values were following a uniform distribution which is not our case. However, as the median is often approximated by the middle of a sample, we use here the same kind of hypothesis.

\item Seeking a pair of estimators with the method of moments involves finding two equations where our sample moment and the distribution one are equated for both orders. From the expression we need to verify, we see that it is easier to use the moments of order $\alpha$ and $2\alpha$.

$$\mu_\alpha = \mathbb{E}(T^\alpha) = m_\alpha = \frac{1}{n}\sum Y_{i}^{\alpha}$$
$$\mu_{2\alpha} = \mathbb{E}(T^{2\alpha})= m_{2\alpha} = \frac{1}{n}\sum Y_{i}^{2\alpha}$$

Since we know the expression of a moment of order k (found in Part 1), we can develop $\mu_\alpha$ and $\mu_{2\alpha}$. The expression is then simplified using the property $\Gamma(n+1) = n!$

$$\frac{1}{n}\sum T_i^\alpha = \beta \frac{\Gamma(\frac{\alpha}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{\alpha}{\alpha}+1+\beta)} = \frac{\beta(\beta-1)!}{(\beta+1)!} = \frac{1}{\beta+1}$$

$$\frac{1}{n}\sum T_i^{2\alpha} = \frac{\Gamma(\frac{2\alpha}{\alpha}+1)\Gamma(\beta)}{\Gamma(\frac{2\alpha}{\alpha}+1+\beta)} = \frac{2\beta(\beta-1)!}{(\beta+2)!} = \frac{2}{(\beta+2)(\beta+1)}$$

By rearranging the terms of our first line, we find an expression for $\hat{\beta}_M$ which is the second equation we were looking for. By injecting this into our second line, we then obtain the first equation to be verified after another rearrangement of terms.

$$\hat{\beta}_M = \frac{n}{\frac{1}{n}\sum T_i^{\hat{\alpha}_M}} - 1$$

$$\left ( \frac{1}{n}\sum T_i^{2\hat{\alpha}_M} \right )  \left (n + \frac{1}{n}\sum T_i^{\hat{\alpha}_M} \right )  \left ( \frac{1}{n}\sum T_i^{\hat{\alpha}_M} \right )^{-2} = 2$$

\item We seek the two parameters $\hat{\alpha}_L$ and $\hat{\beta}_L$ that maximise our likelihood function. 

$$L(T_i|\alpha,\beta) = \prod_{i=1}^{n} f_{\alpha,\beta}(T_i) = \prod_{i=1}^{n} \alpha\beta T_i^{\alpha-1}(1-T_i^\alpha)^{\beta-1}$$

In order to make the following derivations easier, we apply the natural logarithm to our function. This will not change the solution of our maximisation problem as the logarithm is a monotonically increasing function.

$$l(T_i|\alpha,\beta) = \ln(L(T_i|\alpha,\beta)) = \sum_{i=1}^n \left [ \alpha\beta T_i^{\alpha-1}(1-T_i^\alpha)^{\beta-1} \right ]$$

We now derive our function with respect to each of our parameters and find the zeros of these derivatives.

$$ \frac{\partial l(\alpha, \beta)}{\partial \alpha} = \frac{n}{\alpha} + \sum_{i=1}^n \ln(T_i) + (1-\beta) \sum_{i=1}^{n} \frac{\ln(T_i) T_i^\alpha}{1-T_i^\alpha} = 0$$
$$ \frac{\partial l(\alpha, \beta)}{\partial \beta} = \frac{n}{\beta} + \sum_{i=1}^{n} \ln(1- T_i^\alpha)=0$$

As for the previous method, we isolate $\hat{\beta}_L$ in the second line and then inject its expression in the first one. After a rearrangement of terms, we obtain the sought expressions.

$$\frac{1}{\hat{\alpha}_L} + \left ( \frac{1}{\sum_{i=1}^n \ln(1-T_i^{\hat{\alpha}_L})} + \frac{1}{n}\right ) \sum_{i=1}^{n}\frac{\ln(T_i)T_i^{\hat{\alpha}_L}}{1-T_i^{\hat{\alpha}_L}} = - \frac{1}{n} \sum_{i=1}^n \ln(T_i)$$

$$\hat{\beta}_L = \frac{-n}{\sum_{i=1}^n \ln(1-T_i^{\hat{\alpha}_L})}$$

\item We know that our sample follows a $f_{\alpha,\beta}$ distribution and we now have estimators for the parameters $\alpha$ and $\beta$. We can therefore use our relation from Part 1 between a quantile of order p and the distribution parameters to have the expression of a new estimator for $t_{0.95}$.

$$ \hat{q}_M = (1-0.05^{\frac{1}{\hat{\beta}_M}})^{\frac{1}{\hat{\alpha}_M}}$$

$$ \hat{q}_L = (1-0.05^{\frac{1}{\hat{\beta}_L}})^{\frac{1}{\hat{\alpha}_L}}$$

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Part 3 : Simulations}

\begin{enumerate}[(a)]

\item From the generated data,using Part 2, we can get three estimates of $q_{0.95}$.
\\

\begin{center}
    \begin{tabular}{ | l | l | l | }
    \hline
      $\hat{q}_s$ & $\hat{q}_M$ & $\hat{q}_L$
     \\ \hline
     0.7592566 & 0.8061234 & 0.8252839 \\ \hline
    \end{tabular}
\end{center}

\item If we repeat this data generating process N=1000 times (with the same conditions as point (a)), we obtain a sample of size N of  each estimator of $q_{0.95}$. We can now make a histogram and a boxplot of these three samples. 

\begin{figure}[!h]
    \centering
  \includegraphics[width = 10cm]{img/boxplot.png}
  \caption{Boxplot of these three samples}
\end{figure}

As we can see from the figure 2, the data  from the three estimators are very similarly distributed around the median wich equals approximately 0.725. The best estimator seems to be the MLE because the data are the most confined around the median.

\begin{figure}[!h]
    \centering
  \includegraphics[width = 10cm]{img/hist3b.png}
  \caption{Histogram of these three samples}
\end{figure}

As we can see from the figure 3, most of the data are confined around the median. Moreover, the histograms from the the MLE and Method of moments are very similar. 

\item From the samples obtained in (b), we can estimate the bias, the variance and the mean squared error of $\hat{q}_s$, $\hat{q}_M$ and $\hat{q}_L$. 

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
     & $\hat{q}_s$ & $\hat{q}_M$ & $\hat{q}_L$
     \\ \hline
    Bias & -0.008617232 & -0.1517907 & -0.009336601 \\ \hline
    Variance & 0.005044280 & 0.004414109 & 0.003883179 \\ \hline
    MSE & 0.005118536 & 0.004644513 & 0.003970351
 \\
    \hline
    \end{tabular}
\end{center}

The most efficient estimator should be unbiased with smallest variance. So the estimator from the MLE is the best. This can be seen just by analysing the MSE. The best estimator has the lowest mean squared error. Since $\hat{q}_L$ has the smallest MSE, it is the best estimator.
\\

\item We repeat the calculations in (c) nine times for n=40,60,80,100,150,200,300,400,500. We can now compare the biases, the variances and the mean squared errors of $\hat{q}_s$, $\hat{q}_M$ and $\hat{q}_L$ graphically. 

\begin{figure}[!h]
    \centering
  \includegraphics[width = 14cm]{img/3d.png}
  \caption{Plots of the bias, variance, and mean squared error of each estimator.}
\end{figure}

We can conclude from the figure 4 that more we have observations (so n increases) better the estimator is. Again the best estimator here is the MLE because it has the smallest MSE but the curves from the Method of moments are very closed to those from MLE so it is approximately as much as efficient.


\item By comparing the estimator $\hat{q}_L$ and $\hat{q}_s$ according the relation $\sqrt{n}(\hat{q}_L-q_{0.95})$, we can plot historgams for n=20,100,300.

\begin{figure}[!h]
    \centering
  \includegraphics[width = 10cm]{img/3e.png}
  \caption{Histograms of the three samples}
\end{figure}

We can see that when n increases the data are more confined around the median, so the estimator becomes better. Moreover, the median is approximately 0 which means that the estimator $\hat{q}_L$ is very closed to exact value  $q_{0.95}$.


\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Linear regression and ANOVA}

\begin{enumerate}[(a)]

\item From Figure 2.a, we can observe that there is no linear relationship between the fuel consumption (Y) and horsepower (X). Yet, two interpretations can be done from the two other graphs. The first one is that the relation between our Y and W variables is $W = \ln(Y)$. The second one is that there seems to be a linear correlation between our W and X variables. Hence, it is much more reasonable to fit a linear regression model between $\ln(Y)$ and X.

\begin{figure}
    \centering
    \includegraphics[width = 15cm]{img/dataset.eps}
    \caption{Behaviours between the variables}
\end{figure}

\item We seek to find $\hat{\beta}_1$ and $\hat{\beta}_0$, the parameters of the line fitting the best to our data points. 

$$\mathbb{E}(\ln(Y)) = \beta_1 \cdot X + \beta_0$$

To achieve this, we use the method of least squares and therefore use the following expressions:
$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\quad \textup{and} \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

The results are written in Table 1 and shown in Figure 2.(c). The interpretation we can make is that the fuel consumption increases as the horsepower does since the slope $\hat{\beta}_1$ is positive. It is even a exponential increase as we have a linear relation between X and $\ln(Y)$.

\begin{table}[h]
    \begin{centering}
    \begin{tabular}{|c|c|}
    \hline 
    $\hat{\beta}_1$ & 0.0041\tabularnewline
    \hline 
    $\hat{\beta}_0$ & -1.6117\tabularnewline
    \hline 
    \end{tabular}
    \par\end{centering}
    \caption{Coefficients of our linear regression}
\end{table}

\item Let's find a way to associate a mathematical expression to this significance. We choose a method similar to the one used in ANOVA, decomposing the total variance of our dataset into a variance explained by the model and a residual variance. 

\nonumber
\begin{equation} \label{eq1}
\begin{split}
\sum_{i=1}^n(W_i-\bar{W})^2 & = \sum_{i=1}^n(\hat{W}_i-\bar{W})^2 + \sum_{i=1}^n(W_i-\hat{W}_i)^2 \qquad \textup{with} \quad \hat{W}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \\
 \textup{SS Total} & = \textup{SS Regression} + \textup{SS Errors}
\end{split}
\end{equation}

Note that SST and SSE follow a chi-square distribution with n-1 and n-2 degrees of freedom respectively. Hence, we can deduce that SSReg $\sim \chi_1^2$. Using the relationship between chi-square and F distributions, we obtain: $F = \frac{\textup{SSReg}/1}{\textup{SSE}/(n-2)} \sim F_{1,n-2}$ 

This is our indicator: the closer to the infinity, the more significant  the linear effect will be. 
We now construct a F-test with $H_0$ : $F\rightarrow + \infty $ versus $H_a$: $F< +\infty$. The value of our test statistic is F = 623.8537 which is huge. As the distribution function is gathered around numbers under 10, there is very small probability for our observed value to be reached.

Let's confirm our first idea by computing the p-value which is the smallest level of significance $\alpha$ of our test for which the data value indicates that our null hypothesis should be rejected. It can be obtained by integrating our density function between 623.8537 and $+\infty$.

$$\alpha = \int_{623.8}^{+\infty} F(x)_{1,98} dx  = 1- \int_0^{623.8} F(x)_{1,98} dx \approx 0$$

Our result is so tiny that even our matlab tools cannot handle it and approximates it to 0. Based on the following p-value, we can conclude that the linear effect is indeed significant.

\item Let us recall that W is a random variable which is normally distributed with mean $\beta_0 + \beta_1$ and variance $\sigma^2$. The following relation is the best estimator we have to find the $\hat{W^*}$ corresponding to the $\hat{X^*}$ value : the real value is approximated by the expectancy.

$$\hat{W^*} = \hat{\beta}_0 + \hat{\beta}_1 x^*$$

Since $Y = e^{W}$ and that $X^*=1000$, $\beta_1=0.0041$ and $\beta_0=-1.6117$, we find that the expected fuel consumption of Koenigsegg One :1 is $Y^*=$ 12,5209 [l/100km].

We are now interested into finding a $95\%$ prediction interval for this value. We know that the error made using the previous estimator, is normally distributed with a null mean and a certain variance. From this, it can be shown that the following variable T possesses a Student's t distribution.

$$T = \frac{W^*-\hat{W^*}}{S\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}}} \qquad \textup{with} \quad S^2= \frac{\sum_{i=1}^n(W_i-\hat{W}-\bar{(W_i-\hat{W})})^2}{n-1} \qquad \textup{and} \quad S_{xx} = \sum_{i=1}^n (x_i-\bar{x})^2  $$

As in the confidence intervals method, we observe that $P(-t_{\alpha /2} < T < t_{\alpha /2}) = 1-\alpha$

By isolating $W^*$, we obtain :

$$P\left ( \hat{W^*}-t_{\alpha/2}S\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}} < W^* < \hat{W^*}+t_{\alpha/2}S\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}} \right ) = 1-\alpha$$

Using values from our dataset and $t_{\alpha/2} = 1.9845$, we get:

$$P\left (1.7767 < W^* < 3.2781 \right ) = 0.95$$

In terms of Y, the interval $Y \in \left [ 5.91;26.5 \right ]$ will contain the predicted value of $Y^*$ with $95\%$ probability. This result is astonishing since even if $0.95$ is large, we could think that the good alignment of the points on FIGURE 2(c) would lead to a good confidence in our prediction. This surprising length  of interval is due to the standard deviation factor S disturbed by the scattering of the first points.

\item We are now going to test the independence between the fuel consumption\footnote{As asked in the instructions, we will work with the variable W which is directly related to Y.} and the type of car. As the number of samples is important, we expect the average W to be relatively similar for each Z if Z has actually no impact on W. In fact these averages should equalise for a infinite number of sample. Therefore, our test hypothesis is $H_0$ : $ \mu_1= \mu_2 = \mu_3=$.

Using the decomposition of the variance method (ANOVA) (close to what we did in question 4.c), we find a test statistic that follows a F distribution.

\nonumber
\begin{equation} \label{eq3}
\begin{split}
\sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y})^2 & =  \sum_{i=1}^k \sum_{j=1}^{n_i}(\bar{Y}_{i}-\bar{Y})^2 + \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y}_i)^2\\
 \textup{SS Total} & = \textup{SS Treatment} + \textup{SS Errors}
\end{split}
\end{equation}


$$F = \frac{\textup{MST}}{\textup{MSE}} = \frac{\textup{SST}/(k-1)}{\textup{SSE}/(n-k)} \sim F(k-1,n-k)$$

This is a one-tailed test : as $\alpha  = 0.05$, the rejection region is $F_{\textup{obs}} < F_{2,97,0.05} = 3.0902$. The observed value $F_{\textup{obs}}=333.7680$  is far bigger than $F_{2,97,0.05}$ and therefore we reject $H_0$. In fact, it seems that there is a correlation between the fuel consumption and the type of car. This result is not surprising since the average W for each Z are quite different.

$$\bar{W}_{z=1} = -0.5959 \qquad \bar{W}_{z=2} = 0.2811 \qquad \bar{W}_{z=3} = 1.6099$$


\item The assumptions made in the previous answer are as follows:
\\
$\mathbf{Equal} \mathbf{Variance}$ : As this value is unknown for each of our Z, we use the estimator $S^2$ based on the values of our dataset. The resulting estimated variances for each Z are quite similar.

$$S^2_{z=1} = 0.2929 \qquad S^2_{z=2} = 0.2088 \qquad S^2_{z=3}  = 0.2730$$

$\mathbf{Normality}$ : The error $\epsilon_{ij} = Y_{ij} - \mu_i$ follows a normal distribution centered around zero and with variance $\sigma^2$, the value we tried to estimate previously. To verify this assumption, we use a Jarque-Bera test on matlab : this test verifies if the values fit to a normal distribution. The tool returns the test decision with a $5\%$ significance level. Using this test, we conclude that the assumption is true.

$\mathbf{Independence}$ : Our 100 observations are not related to each other meaning the position of one value won't influence the others. In fact, they don't in this case study.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\appendix
\section*{APPENDIX : MatLab Code}

\subsection*{Part 3}
\inputminted{matlab}{Proba3_complet.m}

\subsection*{Part 4}
\inputminted{matlab}{part4.m}
\end{document}
